# Подробный пересказ лекции о нейронных сетях

## Нейронные сети: от Хопфилда до Колмогорова

Лекция начинается с краткого обзора предыдущей темы – нейронной сети Хопфилда и ее способности восстанавливать информацию по контексту. Затем фокус смещается на основную тему – теорему Колмогорова-Арнольда и ее влияние на развитие нейронных сетей.

### Теорема Колмогорова-Арнольда: суть и значение

В 1957 году Андрей Колмогоров и Владимир Арнольд, выдающиеся математики XX века, доказали теорему, которая утверждает, что любую многомерную непрерывную функцию можно представить в виде композиции функций одной переменной и операции сложения.  

Долгое время практическое применение этой теоремы оставалось неясным. Однако в 1987 году Хехт-Нильсен адаптировал ее для нейронных сетей, показав, что перцептрон способен аппроксимировать любую функцию с любой заданной точностью. Это открытие вызвало всплеск интереса к нейронным сетям, поскольку оно означало, что они, теоретически, могут решать любые задачи.

### Неконструктивность теоремы и поиск алгоритма обучения

Несмотря на важность теоремы Колмогорова-Арнольда, она не указывает **конкретный способ** нахождения решения, а лишь утверждает его существование.  На данный момент не существует формализованного алгоритма обучения, основанного на этой теореме, который гарантировал бы сходимость и воспроизводимость результатов. Разработка такого алгоритма – это важная научная задача, решение которой может привести к значительному прогрессу в области нейронных сетей.


## Аппроксимация, интерполяция, экстраполяция и линейная разделимость в нейронных сетях

### Аппроксимация: приближение к истине

**Аппроксимация** – это приближенное представление функции с помощью другой, как правило, более простой функции.  Этот процесс можно проиллюстрировать на примере построения линии тренда в Microsoft Excel.  Например, парабола может быть использована для аппроксимации набора экспериментальных точек.  **Среднеквадратичное отклонение (R-квадрат)** – это показатель, который оценивает качество аппроксимации. Чем ближе R-квадрат к единице, тем точнее приближение.

### Интерполяция и экстраполяция: взгляд внутрь и за пределы

**Интерполяция** и **экстраполяция** – это частные случаи аппроксимации.
* **Интерполяция** – это восстановление значения функции внутри диапазона известных значений.
* **Экстраполяция** – это предсказание значения функции за пределами диапазона известных значений. 

Интерполяция обычно дает более точные результаты, чем экстраполяция, поскольку прогнозирование на много шагов вперед сопряжено с большей неопределенностью.

### Линейная разделимость: простота или сложность?

**Линейная разделимость** – это свойство функции, которое определяет, можно ли разделить точки, принадлежащие разным классам, с помощью плоскости (или гиперплоскости в многомерном случае).  

Например, если вершины трехмерного куба закодированы значениями +1 и -1, то функция, разделяющая вершины с разными значениями, может быть линейно разделимой или нет.  Если провести плоскость невозможно, то функция не является линейно разделимой и для ее аппроксимации потребуется более сложная комбинация разделяющих плоскостей, что ведет к использованию нейронных сетей с одним или несколькими скрытыми слоями.

### Пространство признаков: трансформация данных

Скрытые слои нейронных сетей преобразуют входные данные в новое **пространство признаков**. В этом новом пространстве функция, которая была нелинейно разделимой во входном пространстве, может стать линейно разделимой.  Это преобразование делает данные более структурированными и упрощает задачу классификации.


## Обучающая выборка и эпоха: основа обучения нейронных сетей

### Обучающая выборка: данные для обучения

**Обучающая выборка (датасет)** – это набор данных, который используется для обучения нейронной сети.  Датасет состоит из **экземпляров (строк)**, каждый из которых содержит набор **входных параметров** и соответствующее ему **целевое значение (принадлежность классу)**. 

### Разделение обучающей выборки: обучение, тестирование и валидация

Обучающая выборка обычно делится на три части: 
* **Обучающая выборка**: используется непосредственно для обучения нейронной сети.
* **Тестовая выборка**: используется для оценки качества обученной сети на данных, которые она не видела во время обучения.
* **Валидационная выборка**: используется для настройки параметров сети во время обучения, например, для определения оптимального количества эпох.

### Контрольная выборка: взгляд заказчика

Иногда вместо тестовой выборки используется **контрольная выборка**, предоставленная заказчиком для независимой оценки качества модели. Важно различать эти понятия.

### Размер и качество обучающей выборки: залог успеха

Для эффективного обучения нейронной сети требуется достаточно большой и репрезентативный датасет.  **Ошибочные экземпляры** в обучающей выборке могут негативно влиять на качество обучения, поэтому важно их выявлять и анализировать.

### Эпоха: один шаг по всем данным

**Эпоха** – это один полный проход по всей обучающей выборке.  В течение каждой эпохи нейронная сеть обрабатывает каждый экземпляр обучающей выборки и корректирует свои веса в соответствии с полученным результатом. Количество эпох, необходимых для обучения, зависит от сложности задачи и размера обучающей выборки.


## Валидационная выборка и переобучение: контроль качества обучения

### Валидационная выборка: страж от переобучения

Обучение нейронных сетей с помощью метода обратного распространения ошибки не гарантирует сходимость, поэтому необходимо контролировать процесс обучения и ограничивать его продолжительность. **Валидационная выборка** используется для мониторинга **функции потерь (ошибки)** на данных, которые не участвуют непосредственно в обучении.

### Переобучение:  запоминание вместо обобщения

**Переобучение** – это явление, когда нейронная сеть "запоминает" обучающую выборку, но плохо обобщает на новые, не виденные ранее данные.  Переобучение проявляется в том, что функция потерь на обучающей выборке уменьшается, а на валидационной выборке начинает расти. 

### Предотвращение переобучения с помощью валидационной выборки

Валидационная выборка помогает предотвратить переобучение.  Если ошибка на валидационной выборке начинает расти, то процесс обучения следует прервать, так как это свидетельствует о начале переобучения.

### Ограничения валидационной выборки: не панацея

Валидационная выборка – это важный инструмент, но она не всегда гарантирует нахождение оптимального решения.  Например, если функция является линейно неразделимой и в обучающей выборке присутствуют ошибочные экземпляры, то процесс обучения может не сойтись, даже если ошибка на валидационной выборке остается небольшой. 


## Меры близости: как измерить разницу

###  Меры близости: оценка расстояния

**Меры близости** используются для оценки расстояния между экземплярами данных.  Выбор меры близости влияет на результаты работы нейронной сети.

### Манхэттенское расстояние: прогулка по кварталам

**Манхэттенское расстояние (L1)** вычисляется как сумма модулей разностей координат.  Это расстояние можно представить как длину пути по улицам города, где движение возможно только по горизонтали и вертикали, подобно прогулке по "римским кварталам".

### Евклидово расстояние:  кратчайший путь

**Евклидово расстояние (L2)**  – это корень квадратный из суммы квадратов разностей координат.  Это расстояние соответствует длине прямой линии между двумя точками – кратчайшему пути.

### Другие меры близости: разнообразие вариантов

Существуют и другие меры близости, например, расстояние Минковского четвертого порядка. Выбор конкретной меры зависит от задачи и характера данных.

### Косинусная мера: угол между векторами

**Косинусная мера близости** оценивает угол между векторами в многомерном пространстве.  Эта мера полезна, когда важна не столько абсолютная разница между экземплярами, сколько их  взаимное расположение – направление.

### Косинусная мера в анализе текстов

Косинусная мера часто применяется в задачах анализа текстов, где документы представляются в виде векторов в пространстве слов. В высокоразмерных пространствах косинусная мера  может быть более эффективной, чем евклидово расстояние.


## Гиперболические меры близости: взгляд за пределы эллипсов

### Разнообразие квадратичных мер

**Квадратичные меры**  – это еще один класс мер близости.  Они могут быть как с плюсом, так и с минусом, что существенно влияет на их геометрическую интерпретацию.

### Эллиптические и гиперболические меры: окружности vs. гиперболы

* **Эллиптические меры**  соответствуют окружностям и эллипсам в двумерном пространстве, а в многомерном случае – сферам и эллипсоидам. 
* **Гиперболические меры** соответствуют гиперболам.

### Редкость гиперболических мер

Гиперболические меры близости используются реже, чем эллиптические, но в некоторых задачах они могут быть более эффективными.


##  Проблема "черного ящика" в нейронных сетях:  непрозрачность и невоспроизводимость

### Нейронные сети под критикой

Обучение нейронных сетей с помощью метода обратного распространения ошибки – это непрозрачный процесс, который часто называют "черным ящиком".  Это  вызывает  критику  нейронных  сетей  со  стороны  научного  сообщества.

### Невоспроизводимость:  проблема для науки

**Невоспроизводимость эксперимента**  – это  серьезный  недостаток  в  науке.  Однако  в  области  нейронных  сетей  на  этот  недостаток  часто  закрывают  глаза,  поскольку  главным  критерием  является  эффективность  обученной  сети.

### Причины невоспроизводимости: случайность и локальные минимумы

Невоспроизводимость  обучения  нейронных  сетей  обусловлена  двумя  основными  факторами:
* **Случайное  задание  начальных  весов**:  начальные  значения  весов  нейронной  сети  задаются  случайным  образом,  что  влияет  на  весь  процесс  обучения.
* **Градиентные  методы  оптимизации**:  градиентные  методы  могут  "застревать"  в  локальных  минимумах  функции  потерь,  не  достигая  глобального  минимума.

### Последствия невоспроизводимости: сложности с интерпретацией и отладкой

Невоспроизводимость  обучения  нейронных  сетей  приводит  к  следующим  проблемам:
* **Сложности  с  интерпретацией  результатов**:  трудно  понять,  почему  нейронная  сеть  приняла  то  или  иное  решение.
* **Проблемы  с  отладкой  и  улучшением  модели**:  сложно  найти  причины  ошибок  и  улучшить  работу  сети.


##  Лабораторная работа: исследование логических функций в нейронных сетях

###  Цель работы: нейроны как логические элементы

**Цель лабораторной работы** –  экспериментально  подтвердить,  что  нейроны  в  нейронной  сети  реализуют  логические  функции.

###  Описание задания:  обучение и анализ

**Задание  на  лабораторную  работу  включает  следующие  этапы**:
* **Датасет**:  студентам  предоставляется  датасет  для  обучения  нейронной  сети.
* **Архитектура  сети**:  студентам  предлагается  базовая  архитектура  нейронной  сети,  которую  можно  модифицировать.
* **Функция  активации**:  студентам  предлагается  использовать  различные  функции  активации,  такие  как  тангенс  гиперболический  и  ReLU.
* **Анализ  результатов**:  студентам  необходимо  проанализировать  полученные  результаты  и  убедиться,  что  нейроны  реализуют  логические  функции.

### Демонстрация решения: Python и Keras

В  рамках  лекции  демонстрируется  пример  решения  задания  в  среде  программирования  Python  с  использованием  библиотеки  Keras.  Показывается,  как  обучить  нейронную  сеть  на  предоставленном  датасете  и  проанализировать  полученные  веса  нейронов.

### Многозначные логики: выход за рамки двоичной системы

Анализ  результатов  показывает,  что  нейроны  в  нейронной  сети  могут  реализовывать  не  только  двоичные  логические  функции,  но  и  многозначные  логики.  Функция  активации  после  обучения  может  быть  заменена  на  пороговую  функцию,  что  подтверждает  реализацию  логической  функции  нейроном.

### Смешанные логики:  разные значности на входе и выходе

**Смешанные  логики**  возникают,  когда  нейрон  получает  на  вход  данные  одной  значности,  а  на  выходе  выдает  данные  другой  значности.  В  рассмотренной  нейронной  сети  приводятся  примеры  нейронов,  реализующих  смешанные  логики.

### Дополнительные рекомендации: автоматизация, выбор среды, обсуждение

**Рекомендации  по  выполнению  лабораторной  работы**:
* **Автоматизация  расчетов**:  рекомендуется  написать  программу,  которая  будет  автоматизировать  процесс  обучения  нейронной  сети  и  анализа  результатов.
* **Выбор  среды  разработки**:  работу  можно  выполнять  на  своих  компьютерах  или  использовать  онлайн-сервисы,  такие  как  Google  Colab.
* **Обсуждение  результатов**:  полученные  результаты  можно  обсуждать  с  преподавателем  и  другими  студентами.